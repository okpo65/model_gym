folds: 1
params:
  hidden_size: 256
  dropout_ratio: 0.2

optimizer:
  init_lr: 5e-5
  l2_reg: 2e-3

scheduler:
  gamma: 0.995
  patience: 30
  verbose: 0
  cooldown: 2
  min_lr: 1e-7

earlystopper:
  min_delta: 1e-7
  patience: 10
  verbose: 1

batch_size: 512
iterations: 3000

eval_verbose: 1

path: res/models
name: mlp
result: mlp_4500dae_standard_jb_v2_no_cat # mlp_256hs_dw_v2 # mlp_transformer_dae_1024_dw_v2 # mlp_transformer_dae_1024_dw_v2 # mlp_64dae_256hs_dw
