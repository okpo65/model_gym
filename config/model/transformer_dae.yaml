optimizer:
  init_lr: 3e-5
  lr_gamma: 0.995

scheduler:
  gamma: 0.995

earlystopper:
  min_delta: 1e-7
  patience: 30
  verbose: 1

hidden_size: 1024
num_subspaces: 8
embed_dim: 128
num_heads: 8
dropout: 0.2
feedforward_dim: 512
emphasis: 0.75
mask_loss_weight: 2

noise_ratio:
  num: 0.25
  cat: 0.25

batch_size: 512
iterations: 3000

loss_weight:
  cat: 0.2
  num: 0.8

eval_verbose: 1

path: res/models
name: transformer_dae
result: transformer_dae_1024_dw_v2
